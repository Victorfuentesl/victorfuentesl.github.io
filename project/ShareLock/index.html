<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="We propose a benchmark to test how well language models facilitate generalization in visual tasks. With ShareLock, we introduce an ultra-lightweight CLIP-like vision-language model that achieves competitive multimodal performance with minimal computational resources.">
  <meta property="og:title" content="Better Language Models Exhibit Higher Visual Alignment"/>
  <meta property="og:description" content="An efficient approach to vision-language modeling, achieving state-of-the-art results in low-data regimes with minimal computational cost."/>
  <meta property="og:url" content="https://jonaruthardt.github.io/project/ShareLock"/>
  <!-- TODO Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/ShareLock.png"/>
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="Better Language Models Exhibit Higher Visual Alignment">
  <meta name="twitter:description" content="Efficient vision-language modeling with ShareLock. Competitive performance, minimal cost.">
  <meta name="twitter:image" content="static/images/ShareLock-Twitter.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="vision-language model, ShareLock, CLIP, machine learning, multimodal AI">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>ShareLock: Ultra-Lightweight CLIP-like Vision-Language Model</title>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üïµüèº‚Äç‚ôÇÔ∏è</text></svg>">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/tailwindcss/2.2.19/tailwind.min.css" rel="stylesheet">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Better Language Models Exhibit Higher Visual Alignment</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://jonaruthardt.github.io" target="_blank">Jona Ruthardt</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://gertjanburghouts.github.io" target="_blank">Gertjan J. Burghouts</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://sergebelongie.github.io" target="_blank">Serge Belongie</a><sup>3</sup>,</span>
                    <span class="author-block">
                      <a href="https://yukimasano.github.io" target="_blank">Yuki M. Asano</a><sup>1</sup>
                    </span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>1</sup>FunAI Lab, University of Technology Nuremberg<br>
                      <sup>2</sup>Intelligent Imaging, TNO<br>
                      <sup>3</sup>Department of Computer Science, University of Copenhagen<br>
                    </span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2410.07173.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/JonaRuthardt/ShareLock" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2410.07173" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- TL;DR Section -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">TL;DR</h2>
        <div class="content has-text-justified">
          <p>
            We explore how much text-only language models naturally align with the visual world and find that off-the-shelve LLMs effectively encode visually relevant semantics. With <b>ShareLock</b>, we leverage these insights in an ultra-lightweight vision-language model that achieves <b>52% zero-shot accuracy on ImageNet</b> while trained on just 563k image-caption pairs in <b>less than 1 GPU hour</b>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Visual Generalization of Language Embeddings</h2>
        <div class="content has-text-justified">
          <p>
            We measure the degree to which language representations facilitate generalization in the vision domain in a CLIP-like setup. By freezing the language model and strictly controlling the concepts seen during training and inference, we can isolate the visual information encoded by the language model. 
            Intriguingly, we find that the general model performance (measured by MMLU-Pro) correlates strongly with the model's visual generalization performance. This suggests that better language models naturally align with the visual world.
          </p>
        </div>
        <figure class="has-text-centered">
            <img src="static/images/llm_performance.png" alt="Scaling Laws Diagram" style="max-width: 75%; height: auto; display: block; margin-left: auto; margin-right: auto;">
          <figcaption>Figure 1: Visual Generalization of Language Models vs. MMLU-Pro Scores</figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- Key Findings -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Key Insights</h2>
        <div class="grid md:grid-cols-3 gap-4">
          <div class="p-4 bg-gray rounded-lg", style="background-color: #f5f5f5;">
            <h3 class="text-l font-semibold mb-4">LLMs Encode Visual Knowledge</h3>
            <p>LLMs can effectively absorb and interpolate substantial amounts of factual knowledge about the visual world.</p>
          </div>
          <div class="p-4 bg-gray rounded-lg", style="background-color: #f5f5f5;">
          <h3 class="text-l font-semibold mb-4">Decoder-Based Models Excel</h3>
            <p>Decoder-based language models consistently outperform encoder architectures in visual tasks. With Gemma-2 (9B), an off-the-shelve LLM represented visual information best. </p>
          </div>
          <div class="p-4 bg-gray rounded-lg", style="background-color: #f5f5f5;">
              <h3 class="text-l font-semibold mb-4">LLM Capability Predicts Visual Performance</h3>
              <p>We discovered a strong correlation between a language model's general capabilities and its visual understanding, with a Pearson correlation of 0.768.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Model Architecture Section -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Model Architecture</h2>
        <figure class="has-text-centered">
          <img src="static/images/Diagram ShareLock.png" alt="Model Architecture Diagram" style="max-width: 100%; height: auto;">
          <figcaption>Figure 2: Diagram of the ShareLock model architecture</figcaption>
        </figure>
        <br>
        <div class="content has-text-justified">
          <p>
            ShareLock adopts a modular design that combines frozen vision and language models to extract high-quality unimodal features. These features are then aligned in a shared embedding space through a lightweight, learnable projection head. The projection network on top of the frozen language representations is optimized using a contrastive loss, ensuring that image-text pairs are effectively matched in the latent space. This architecture allows efficient training on limited data while maintaining competitive performance.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Leveraging LLMs in CLIP-like VLMs</h2>
        <div class="content has-text-justified">
          <p>
            In ShareLock, we leverage the strong visual alignment of LLM representations by combining them with a frozen vision model. This allows us to achieve performance in vision tasks that are competitive with OpenAI's CLIP model, despite our models being trained with a fraction of the data and compute. We compare ShareLock trained on the CC12M dataset to CLIP and LiT models trained on the same data (for more datasets, see our paper). The following sections highlight ShareLock's performance in various vision tasks.
          </p>
        </div>
        <h3 class="title is-4 has-text-centered">Classification Abilities</h3>
          <div class="content has-text-justified">
            <p>
              Thanks to the strong LLM representations, ShareLock excels in vision tasks like image classification. It achieves 62.0% accuracy on ImageNet-1k, surpassing CLIP (41.6%) and LiT (59.9%) trained on the same data. It also improves the robustness on ImageNet-R and ImageNet-A, handling out-of-distribution images better with less training data, even when compared to OpenAI's CLIP. While fine-grained tasks like Aircraft remain challenging in low-data regimes, ShareLock consistently outperforms models trained on the same data.
            </p>
          </div>
          <figure class="has-text-centered">
            <img src="static/images/classification.png" alt="Diagram comparing the image classification abilities of VLMs" style="max-width: 80%; height: auto; display: block; margin-left: auto; margin-right: auto;">
            <figcaption>Figure 3: Multi-lingual generalization abilities of VLMs trained on CC12M.</figcaption>
          </figure>
        <h3 class="title is-4 has-text-centered" style="margin-top: 2rem;">Multi-Lingual Understanding</h3>
          <div class="content has-text-justified">
            <p>
              Most VLMs struggle with non-English languages because most training data is in English. ShareLock overcomes this limitation by leveraging the multilingual strengths of LLMs. Even with fewer training samples, ShareLock significantly outperforms traditional models like CLIP in languages like Chinese (38.7% vs. 1.4% accuracy) and Japanese (19.8% vs. 4.1%). This makes ShareLock especially powerful for low-resource languages, where high-quality multimodal data is scarce.
            </p>
          </div>
          <figure class="has-text-centered">
            <img src="static/images/multilingual.png" alt="Diagram comparing the multi-lingual generalization abilities of VLMs" style="max-width: 80%; height: auto; display: block; margin-left: auto; margin-right: auto;">
            <figcaption>Figure 4: Multi-lingual generalization abilities of VLMs trained on CC12M.</figcaption>
          </figure>
          <h3 class="title is-4 has-text-centered" style="margin-top: 2rem;">Compositional Reasoning</h3>
          <div class="content has-text-justified">
            <p>
              Understanding fine-grained linguistic differences remains a challenge for VLMs. While ShareLock improves image selection accuracy over OpenAI's CLIP (12.5 vs. 10.8), it still struggles with nuanced compositional reasoning, as seen in benchmarks like Winoground. This suggests that the conventional contrastive vision-language alignment on web-based captions may not be sufficient for more complex tasks.
            </p>
          </div>
          <figure class="has-text-centered">
            <img src="static/images/winoground.png" alt="Scaling Laws Diagram" style="max-width: 80%; height: auto; display: block; margin-left: auto; margin-right: auto;">
            <figcaption>Figure 5: Compositional reasoning of VLMs trained on CC12M on Winoground.</figcaption>
          </figure>
        <h3 class="title is-4 has-text-centered" style="margin-top: 2rem;">Data Scaling</h3>
        <div class="content has-text-justified">
          <p>
            Compared to prior CLIP-like models, ShareLock clearly exhibits more favorable properties in highly data-constrained training regimes, as depicted in the Figure below. When training from scratch, vanilla CLIP models require orders of magnitude more data to achieve similar performance. 
          </p>
        </div>
        <figure class="has-text-centered">
          <img src="static/images/scaling_laws_dataset_size.png" alt="Scaling Laws Diagram" style="max-width: 80%; height: auto; display: block; margin-left: auto; margin-right: auto;">
          <figcaption>Figure 6: Scaling laws of various CLIP-like models.</figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- Results Highlights -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Primary Benefits</h2>
        <div class="grid md:grid-cols-2 gap-8">
            <div>
                <h3 class="text-xl font-semibold mb-4">Efficient Performance</h3>
                <p class="mb-4">ShareLock achieves comparable or better performance than CLIP while using:</p>
                <ul class="list-disc pl-6 space-y-2">
                    <li>Only 8.5M training samples (vs 400M)</li>
                    <li>Significantly fewer compute resources</li>
                    <!-- <li>Significantly fewer GPUs</li> -->
                    <li>Efficient feature precomputation</li>
                </ul>
            </div>
            <div>
                <h3 class="text-xl font-semibold mb-4">Robust Generalization</h3>
                <p class="mb-4">ShareLock demonstrates strong performance across:</p>
                <ul class="list-disc pl-6 space-y-2">
                    <li>Multiple languages</li>
                    <li>Out-of-distribution scenarios</li>
                    <li>Fine-grained classification tasks</li>
                </ul>
            </div>
        </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <div class="columns is-centered">
      <div class="column is-four-fifths">

        <h2 class="title">BibTeX</h2>
        <pre><code>@article{ruthardt2024sharelock,
      title={Better Language Models Exhibit Higher Visual Alignment},
      author={Jona Ruthardt and Gertjan J. Burghouts and Serge Belongie and Yuki M. Asano},
      journal={arXiv preprint arXiv:2410.07173},
      year={2024}
    }</code></pre>
      </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container is-max-desktop">
    <!-- <div class="columns is-centered"> -->
      <!-- <div class="column is-four-fifths"> -->
        <div class="content">

          <p style="color: gray;">
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank" style="color: gray;">Academic Project Page Template</a>.
            <br> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank" style="color: gray;">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
        </div>
      <!-- </div>
    </div> -->
  </div>
</footer>

</body>
</html>
